\documentclass[../main.tex]{subfiles}

\begin{document}

\subsection{Simulation and reinforcement learning}

The simulation and reinforcement learning are critical
components to achieve objective \ref{obj:drl}.
This part of the project starts with receiving
information about the targets' count, distribution
and mobility pattern
and ends with producing a file containing the weights
for the \gls{drl} model.
In our work described below, we have assumed certain
information about the targets but that shall change
with clients and types of the target visitation.

We have represented the targets in the simulation
as green 3D flat boxes
as can be seen in \cref{fig:one-target}. 
The basic shape and the colour green were created by 
defining a model 
in the \gls{sdf}
including its visual, collision and inertial properties.
Using the \textit{libgazebo7} development package, 
we have written a Gazebo plugin 
in C++ to describe custom properties and  
behaviour of the model.
Developing a Gazebo plugin is not a straightforward
process in our case, as explained later, due to 
the incompatibility between the Sphinx plugins and
our \textsc{os}.
In spite of this, we have managed to program the targets' 
main type of behaviour which is its mobility pattern.
For the proof of concept, 
it has been set to be a straight line movement.
The \texttt{population} tag in \gls{sdf} was used 
to place the required number of targets (in this case 20)
following the assumed uniform probability distribution 
function supplied by the
client as shown in \cref{fig:target-population}.
Nevertheless, in the final implementation, the locations 
and the mobility pattern of the targets cannot be uniform
and must follow a certain distribution so that the learning
of the \gls{rl} agent can converge.

\begin{figure}[b]%
    \centering
    \subfloat[one target]{
    {\includegraphics[width=7cm]{one-target} }
    \label{fig:one-target} }%
    \hspace{0.5cm}
    \subfloat[a population of targets]{
    {\includegraphics[width=7cm]{population} }
    \label{fig:target-population} }%
    \caption{%
        \protect\subref{fig:one-target} 
        A target represented as a model in Sphinx and
        \protect\subref{fig:target-population} 
        the target models are scattered across
        the ground following a set distribution.}%
    \label{fig:target}%
\end{figure}

The 3D model, properties and behaviour
of the \anafi drone is provided by Parrot in the 
form of a firmware. 
Both the drone's and the target's models were spawn
in Sphinx using the \gls{sdf} language.
With Sphinx running, the simulated \anafi drone 
started using
a virtual ethernet interface 
(with an \textsc{ip} of 10.202.0.1 by default) 
on the host computer.

Sphinx publishes data about the simulated drone,
one of which is its pose relative to
the world's origin that we have set to be the same
as the drone's take-off point. 
To read this positional data from a Python script, we 
subscribed to the Gazebo topic 
\texttt{/gazebo\-/default\-/pose\-/info}
in a separate thread so that the control signals
will not be blocked while reading the drone's positions.
In addition to receiving data from Sphinx, we have 
sent data to it using the \textsc{json-rpc} standard
with an instruction to disable
the simulation of the \anafi drone's battery
for the \gls{rl}'s purposes. This is important because
if the battery is simulated, the drone will 
perform badly while running
out of battery and after around 20 minutes, 
the drone will stop working and the whole system
needs to be re-spawn. For this reason, the drone's
battery was not simulated.

As for sending movement commands to the drone,
the Parrot Olympe was used.
In an \textsc{ide}, the Olympe library was imported
in a Python script. The library allowed us to connect
to the simulated drone by specifying the drone's
\textsc{ip}. Once the connection was established, 
we could send the Olympe control commands to the drone
to execute the visitation task. Which command to use
depends on the reinforcement learning.

The reinforcement learning was aided 
by another library called \gym.
We used it to facilitate developing the
\gls{drl} algorithm we are implementing and to 
teach the drone how to achieve the 
target visitation mission.
Specifically, \gym provides an abstract class 
called \texttt{Env}
which we have inherited and overridden the 
required unimplemented methods.
The action space is defined in the subclass, and its
elements call the necessary Olympe functions. 
The space consists of 9 movements: 
forward, backward, left,
right, forward-left, forward-right, backward-left,
backward-right and no-operation.
Similarly implemented is the state space 
which is composed of the 25 cells
making up the grid on the ground. 
The relationship and interaction between \gym, Olympe
and Sphinx are shown in \cref{fig:sim-flowchart}.

\begin{figure}[p]
    \centering
    \includegraphics[width=1.0\textwidth]{sim-flowchart}
    \caption{A flowchart showing the training process
        of the reinforcement learning in the project
        and the interactions between the libraries
        and programs.}
    \label{fig:sim-flowchart}
\end{figure}

A crucial element of any \gls{rl} model
is the reward function. In this project, it
is the number of new targets that the drone has captured
in the cell that it has moved to. We have made the drone
to take a picture of the cell underneath it after 
every move and we have used object detection to
identify the targets and count their quantity. 
In addition to object detection which is explained
in the succeeding \cref{sec:obj-detection}, developing this
aspect of the simulation involved 
setting the camera and position of the drone to ensure
the rewards are valid and accurate.

\Cref{fig:fov} shows the view from the drone's
simulated camera. The altitude was set such that
the \gls{fov} covers more than the cell underneath
but not more than half of the adjacent cells.
The reason for the larger coverage is that
the \anafi drone only uses a regular \gls{gps}.
This means that its coordinates are accurate only
to \SIrange{3}{5}{meters}. However, in our scaled-down
prototype, moving to another
cell involves
movements of less than \SI{2}{meters}.
To minimise the errors and ensure that the current
cell will always be in the \gls{fov} even if the
movement is off by few centimeters, we made the camera
cover an area bigger than a cell. 
This however results in a more difficult
object detection because in addition to their 
\id's, we need to also distinguish between
the targets that are within the cell and 
those outside of it.
Another related question that has arisen is if we 
should consider the target that falls on the border 
as a reward such as that in \cref{fig:cell-one}. 
We have decided that we would do so if
the majority part of the target is inside the current
cell, although we realised that this will make the
object detection much harder.

\begin{figure}[tb]%
    \centering
    \subfloat[zero rewards]{
    {\includegraphics[width=4.5cm]{cell-empty} }
    \label{fig:cell-empty} }%
    \hspace{0.2cm}
    \subfloat[one reward]{
    {\includegraphics[width=4.5cm]{cell-one} }
    \label{fig:cell-one} }%
    \hspace{0.2cm}
    \subfloat[two rewards]{
    {\includegraphics[width=4.5cm]{cell-two} }
    \label{fig:cell-two} }%
    \caption{%
        \protect The \gls{fov} of the \anafi drone
        during the training in the simulation
        showing multiple scenarios that the drone
        may encounter at each cell.}%
    \label{fig:fov}%
\end{figure}

A critical problem we have been facing 
is how to distinguish
between the targets. This is a required task in the
project's \gls{rl} training since 
the \gls{drl} model's state
includes the information about whether or not
the targets forming the reward have been seen 
during the current episode or not. 
At first, we came up with the idea of distinguishing them
with colours. However, it is not scalable 
as the 13\textsuperscript{th}
target and above will have to use 
one of the already taken colours
except with different shades. The bigger the number of
targets in the training, the harder it is to
distinguish between them in the object detection part
thus causing errors and inaccuracy. Hence, we decided
to use numbers by displaying a unique \id
to differentiate them, but
Sphinx does not provide that option by default.
A way to get around this is to develop a Gazebo plugin,
but this needs to be done using the \textit{libgazebo7} running
on Ubuntu 16.04 as opposed to Ubuntu 18.04, which we 
have chosen to use because the former 
is no longer supported.
We further solved this problem by using the Ubuntu 16.04
Docker container to write the plugin. While we managed
to successfully made the plugin, it unfortunately still
could not be used on our machine because \textit{libgazebo7}
uses some shared libraries whose versions can be only 
be run on Ubuntu 16.04. One such culprit is the 
\textit{libprotobuf.so.9} from the Google Protocol Buffer 
(Protobuf) used heavily by Sphinx but cannot be compiled
in Ubuntu 18.04. Consequently, 
to solve the problem of making each target distinct, 
we have decided that we would wait for the new
version of Sphinx running on the latest Gazebo
which Parrot will release soon. In case that does
not happen,
we will resort to installing Ubuntu 16.04 despite its
obsolescence to remove the incompatibility issues.

We have also done a preliminary test to see if
we could use an \gls{rl} algorithm
to teach the \anafi drone to fly to a certain
point in space. We used the Trust Region Policy
Optimization (\textsc{trpo}) from the Stable
Baselines3 (\textsc{sb}3) with similar libraries
as described above but continuous state and action spaces.
After \SI{10000}{steps} and \SI{2}{hours}
of training, by using the file containing the weights 
produced at the end of the training,
the drone was able to accurately
fly to the target destination. 

Having proven the feasibility of the physics simulation
and the \gls{rl} training through Sphinx, Olympe, and \gym
to convert client's target information to
a set of \gls{drl} model weights, 
we are confident that the final implementation 
in terms of the simulation and the \gls{rl} components
of the project will be successful in Senior 2.

\subsection{Object detection}\label{sec:obj-detection}
%
%
One of the essential parts in our project is the object
detection model. Object detection is a new computer vision
technique which refers to the computer's ability to 
identify and locate objects in a certain area given a training dataset 
of images or videos. It relies on machine learning and different 
sub-fields of it to train and deploy models in various use cases. 
We started our work with the object detection model right 
after creating the virtual environment in which we created
some targets as shown 
in \cref{fig:one-target,fig:target-population}. These 
targets are easily identifiable by humans, but a mere
machine could not see and separate them from the 
environment around it.
 
After creating the virtual environment, we started taking random 
screenshots all around the virtual area. These screenshots 
might have a lot of targets within their frame or no targets at all. 
This randomness is excellent for the model training as it gives the 
model many different cases as illustrated in 
\cref{fig:labeling-process-1,fig:labeling-process-2}.
We took around 300 screenshots to create a dataset of images and
start the labeling process. For labeling we used Roboflow, which is a 
very useful online tool that allowed us to work as a team and label all
300 images together systematically and consistently to avoid any 
confusion in the model training process. The labeling process must be 
clear and mistake-free, which means that all the boxes created around 
the targets must be precise and exactly surrounding targets without 
including many pixels from the surrounding environment.

\begin{figure}[tbp]%
	\centering
	\subfloat[ten targets within the frame]{
		{\includegraphics[width=6cm]{labeling-process-1} }
		\label{fig:labeling-process-1} }%
	\hspace{0.5cm}
	\subfloat[four targets within the frame]{
		{\includegraphics[width=6cm]{labeling-process-2} }
		\label{fig:labeling-process-2} }%
	\caption{%
		\protect\subref{fig:labeling-process-1} 
		Labeling 10 targets and
		\protect\subref{fig:labeling-process-2} 
		labeling 4 targets.}%
	\label{fig:labeling-targets}%
\end{figure}

For the model training, we relied on \textsc{yolo}v5 which is an open
source family of object detection models and architectures built in 
PyTorch. \textsc{yolo}v5 is based on deep learning schemes such as 
\gls{cnn} to train models given large datasets. There are different
versions of \textsc{yolo}v5. The ones we decided to be 
the most suitable for our project are either 
a compact version that does not require a lot of processing 
power, a medium version, and a full one that
is more suitable for large datasets. We trained our model many times 
using different hardware and \textsc{gpu}s. We also tried performing 
data augmentation on our dataset using built in methods in Roboflow 
which allowed us to make the dataset larger and enhance the performance 
of the model. The data augmentation process resulted in creating three 
images for each image that we have. In that process, images are mixed
together and shuffled or put in different directions with an amount of
rotation that will make the image look new to the model, 
thus training it better.

The tool we used to run the model training was Google 
Colab which allowed us to either use our personal hardware or 
make use of external resources allocated specifically for research 
purposes by Google itself. In Google Colab, we can choose the runtime 
type to either be the internal \textsc{cpu}, \textsc{gpu} or external 
resources from Google. A remote connection was made to Google's server 
and we trained our models using an excellent \textsc{gpu} 
allocated by Google called \textsc{nvidia} Tesla-K80 which is a \textsc{gpu} 
accelerator with 560 \textsc{mh}z core, 10 \textsc{gh}z effective memory clock, and 24 \textsc{gb} 
\textsc{gddr5} v\textsc{ram}.

The results we got were better than what we got using our personal 
\textsc{gpu}s. Higher precision and confidence intervals were 
accomplished, but it was still not the best solution that could be made. 
Later, we started tweaking the model and changing all the possible 
variables to see how the model performs. Tweaking the model involves 
changing many things, such as the batch size, the number of epochs, the 
learning rate, the optimizer, and other small changes that resulted in 
different outcomes. 

In Senior 2, for further improvements in the project, we will run the model 
on Qatar University's server once it is working properly and we have access 
to it. This will give us more options to see how the model performs and decide the 
best possible model for our project.

\subsubsection{\gls{ml} performance evaluation}

\begin{figure}[bp] 
	\centering
	\includegraphics[width=0.8\textwidth]{model-1-performance} 
        \caption{Model No. 1 (small-sized model) performance.} \label{fig:model-1-performance} 
\end{figure}

\begin{figure}[tbp] 
	\centering
	\includegraphics[width=0.8\textwidth]{model-2-performance} 
        \caption{Model No. 2 (large-sized model) performance.} \label{fig:model-2-performance} 
\end{figure}

\begin{figure}[tbp] 
	\centering
	\includegraphics[width=0.8\textwidth]{model-3-performance} 
        \caption{Model No. 3 (medium-sized model) performance.} \label{fig:model-3-performance} 
\end{figure}

The main evaluation metrics are the precision and recall
as seen in \cref{fig:model-1-performance}.
A true positive occurs when the model predicts the positive class correctly, whereas 
a true negative is when the model predicts the negative class correctly. 
False positives and false negatives represent false predictions of the model 
where it mispredicted the positive and negative classes respectively.
The precision is an essential indicator of a model's performance as it shows 
how precise the predictions are by quantifying the number of correct positive predictions. 
It is calculated by the following \cref{eq:precision}. 
%
\begin{align}
    Precision =  \frac{True\ Positives}{True\ Positives + False\ Positives} \label{eq:precision}
\end{align}

Another important metric is the recall value, which shows how many of the 
true positives were found and classified correctly, it is calculated by \cref{eq:recall}.
%
\begin{align}
    Recall =  \frac{True\ Positives}{True\ Positives + False\ Negatives} \label{eq:recall}
\end{align}

From \cref{fig:model-1-performance}, we observe that both the precision and recall start low 
at the beginning, and increase as more epochs pass. By the hundredth 
epoch, both precision and recall are at a high value which is very close 
to one.  

The last metric is the mean average precision (m\textsc{ap}). It is an excellent metric
to compare between object detection models that are running on the same dataset. It measures
how exact the prediction bounding box of the model compared to the correct
bounding box in the dataset. An m\textsc{ap}\_0.5 sets 0.5 as the threshold of overlap 
between the two boxes. As illustrated in 
\cref{fig:model-1-performance,fig:model-2-performance,fig:model-3-performance}, this metric is very high and close to one in all 
three models 
because only an overlap of 0.5 is required to make the box count as a true positive.
However, when we increase the threshold to be 0.5:0.95 with a step size of 0.05, we
see that the value of m\textsc{ap}\_0.5:0.95 is not so high as the m\textsc{ap}\_0.5 because higher precisions of overlap
between the true box and the predicted box is required to make it count as a true
positive. From \cref{tab:model-comparison}, we can see that the third model was better 
than the other models in the m\textsc{ap}\_0.5:0.95 metric.

For the first model, we used the small version of \textsc{yolo}v5 which is 
more \textsc{gpu} friendly and takes a shorter training time.
We made use of data augmentation for this model which improved its result.
The batch size was sixteen to avoid overloading the \textsc{gpu}, 
and we ran it for a hundred epochs only.

\begin{table}[bp]
	\begin{center}
		\caption{A comparison between all three models and their outputs.}
		\label{tab:model-comparison}
                \begin{tabularx}{\textwidth}
                    {p{1.3cm} X X X X X} 
			\toprule
			\textit{Model} & \textit{\textsc{yolo}v5\newline version} & \textit{Epochs} & 
                        \textit{Data\newline Augmentation} & \textit{m\textsc{ap} 0.5:0.95} & \textit{Performance Rating}\\
			\midrule
			 1 & Small & 100 & Yes & 0.81 & Moderate \\
			 2 & Large & 150 & No  & 0.70 & Bad \\
			 3 & Medium& 150 & Yes & 0.87 & Excellent \\		
			\bottomrule
		\end{tabularx}
	\end{center}
\end{table}

The second model performed similar to the first model in terms of simple 
classification and detection cases, but the results were not as expected and there 
were many sharp variations in the precision, recall, and m\textsc{ap} especially around the 
hundredth epoch which clearly shows that something was wrongly configured in this model
even though it ran on the same \textsc{gpu} Tesla K80.
The mistake that was made is that we used the large \textsc{yolo}v5 on a small dataset
without any data augmentation. We also ran it for 150 epochs only which is not enough nor 
suitable for the large \textsc{yolo}v5 version (should be 300 epochs).

Finally, the third model was the best optimization between the two previous models. 
We used the \textsc{yolo}v5 medium version, performed data augmentation, and ran it for
150 epochs. This resulted in a steady rise in the precision as shown in 
\cref{fig:model-2-performance} without many oscillations which means a smooth learning
process unlike what happened in model 2. 
The third model also outperformed both previous models as it scored the highest value 
of m\textsc{ap}\_0.5:0.95, which is 0.87 as shown in \cref{tab:model-comparison}. 

\subsection{Physical components}

The most critical parts in the hardware
architecture are the Raspberry Pi and the drone. 
We started by configuring the Raspberry Pi micro\textsc{sd} card,
flashed it with Ubuntu 18.04 desktop version,
and installed Parrot Olympe successfully. 
Then we plugged in the Wi-Fi dongle adapter
in a \textsc{usb}~2.0 port in the Raspberry Pi, 
and the \textsc{os} discovered it automatically. 

Now everything is ready to test the 
\anafi drone, so we have connected the
Raspberry Pi to the drone using built-in WiFi
to allow Raspberry to send/receive control and status 
instructions to the drone. 
For connecting Raspberry Pi and the laptop, 
we used the external WiFi adapter and turned 
on the hotspot feature to create an access point,
and this made the process so convenient because 
once the Raspberry Pi boots up, it turns on the 
access point, and the user can connect to it 
easily and execute scripts using \textsc{ssh} 
protocol or open the graphical interface 
control using the \textsc{vnc} feature.

We executed the takeoff and move forward scripts,
and it worked successfully, so we got the green 
light to continue. 
Regarding the power supply we were thinking 
of modifying the drone's battery by removing the 
plastic shield because of the drone's payload restrictions 
and limitations, but everything changed after testing 
the drone's maximum payload. As shown in \cref{fig:payload}
we attached \SI{190}{grams} of Raspberry Pi and Arduino boards
to see the effect on the performance and flight and
battery drain percentage. 
The drone has taken off  successfully, 
and the flight was normal but, we faced an expected 
battery drain that went from 
\SI{100}{\percent} to \SI{90}{\percent} in 
\SI{1.5}{minutes}.
This means the drone can fly up to 
\SI{15}{minutes} which is \SI{10}{minutes} 
less than the default battery time 
which will be considered in design constraints.
In our case, the total weight of our parts is equal
to \SI{170}{grams} which will add small time
to the battery life.

\begin{figure}[bp]
	\centering
	\includegraphics[width=0.6\textwidth]{payload.png}
	\caption{The payload of the drone.}
	\label{fig:payload}
\end{figure} 


We have done another experiment regarding the response time 
of the drone after sending a command to it from the Raspberry Pi. 
Calculating the response time was by starting a timer when 
sending the command and stopping it when the 
state of the drone changes. 
The experiment was done on simulation using Sphinx,
then we have moved to the real world using the \anafi drone.
From \cref{tab:respone-time}, 
we can see that all the results are less than \SI{1}{second}
which satisfied our response time constraint.
Another thing we can conclude from the table is that 
the response time of the real world is less than 
the simulation, and this is predictable since the simulation
depends on the \textsc{gpu} power and processing time. 

\begin{table}[tbp]
	\centering
	\caption{The average response time of the drone after sending a control command.}
	\label{tab:respone-time}
        \begin{tabularx}{0.7\textwidth}{ X c c }
		\toprule
		\textit{} & \textit{Simulation} & \textit{Real-world}\\ \midrule
		Motor ramping for takeoff (seconds)  & 0.3506 & 0.0323     \\
		Move while flying (seconds) & 0.7441  & 0.1511   \\
		\bottomrule
	\end{tabularx}
\end{table} 

For the Raspberry Pi power supply, we have ordered it 
online because it is not available locally and 
it arrived after 17 days at the post office.
Regarding the connection, there are 
two different options to connect as shown in \cref{fig:connection}.
For option A we can use USB-A to power the Raspberry directly without
need to solder any wires,
but there is some internal resistance.
In option B it power the Raspberry Pi through the \textsc{gpio} 
interface by soldering two wires to the power board and 
connect to pin 4 and 6 in the Raspberry Pi board,
this option has small internal resistance compared to option A.
For simplicity, we will choose option A and we will 
follow the instruction of the manufacture by choosing 
the thickest and shortest possible USB cable to reduce 
the cable loss and voltage drops~\cite{makerfocus}.

 \begin{figure}[p]
 	\centering
 	\includegraphics[width=0.5\textwidth]{connection.png}
 	\caption{The Raspberry Pi and power board connection.}
 	\label{fig:connection}
 \end{figure}   

Since all the components are available as shown 
in \cref{fig:components}, we can now assemble 
all the parts in one piece and start testing the whole system.
We have used the zip tie to hold the components and to
make sure everything sticks together. After trying to use the 
nylon straps tape to attach the components to the drone 
we have figured out that it blocks the fan 
and sensors in the bottom of the drone, 
so normal wires attached to the sides of the drone were 
used as a temporary solution. Also, we are considering 
designing a 3D printed holder to make the process more 
flexible and convenient in future advancements but for now,
we will stick to the simpler way. As shown in 
\cref{fig:full-hardware} all the parts are in one system 
and test flight was smooth and everything works perfectly. 

 \begin{figure}[p]
	\centering
	\includegraphics[width=0.5\textwidth]{components.jpg}
	\caption{All components needed.}
	\label{fig:components}
\end{figure}

  \begin{figure}[p]
 	\centering
 	\includegraphics[width=0.5\textwidth]{full-hardware.jpg}
 	\caption{Full hardware system.}
 	\label{fig:full-hardware}
 \end{figure}  

\end{document}
