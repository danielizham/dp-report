\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../images/}}}

\begin{document}

\begin{newrequirements}
    \begin{todolist}
    \item[\done] Actual Design description with pictures 
        and diagrams. E.g., a “wiring diagram” 
        of the implemented hardware can be 
        added. 

	\item[\done] Actual images of various modules must 
        be included wherever possible. 
        Otherwise, at least the images of 
        various aspects of the completed design 
        must be shown. 

    \item[\done] List the different tools and framework 
        used for the implementation. 

    \item[\done] Discuss any novel aspects of your 
        implementation (if applicable). You may 
        link this aspect of your design to the 
        comparison table at the end of 
        literature review and elaborate on the 
        steps taken in achieving these 
        novelties in your design. 

    \item [\done] Discuss the challenges encountered 
        during the implementation and how they 
        were addressed. 

    \item [\done] You may organize any of the above 
        recommended points as subsections 

    \end{todolist}
\end{newrequirements}
In this section are going illustrate the actual design 
implementations, to achieve the design solution.
\subsection{Hardware}

The most critical parts in the hardware
architecture are the Raspberry Pi and the drone. 
We started by configuring the Raspberry Pi micro\textsc{sd} card,
flashed it with Ubuntu 20.04 desktop version,
and installed Parrot Olympe successfully in virtual environment. 
Then, we plugged in the Wi-Fi dongle adapter
in a \textsc{usb}~2.0 port in the Raspberry Pi, 
and the \textsc{os} discovered it automatically. 
There were some challenges in setting up the connection automatically 
once the Raspberry Pi boots up. We discovered that some packages were not installed properly,
so we reinstalled the whole system again, and the problems has been addressed. 


Regarding the power supply, we were thinking 
of modifying the drone's battery by removing the 
plastic shield because of the drone's payload restrictions 
and limitations, but everything changed after testing 
the drone's maximum payload.
As far as the connection, there are 
two different options as shown in \cref{fig:connection}.
For option A we can use \textsc{usb}-A to power the Raspberry directly 
without needing to solder any wires, but there is some internal resistance.
In option B, it powers the Raspberry Pi through the \textsc{gpio} 
interface by soldering two wires to the power board and 
connecting to pin 4 and 6 in the Raspberry Pi board.
This option has smaller internal resistance compared to option A.
For simplicity, we chose option A and   
followed the instruction of the manufacturer by choosing 
the thickest and shortest possible \textsc{usb} cable to reduce 
the power loss and voltage drops~\cite{makerfocus}.

As shown in \cref{fig:connection} We have printed the 3d 
parts, and because we need lightweight and strong parts, 
we have used PETG material with a 45\% fill rate, 
then we used superglue to stick all the parts together 
and make sure it's one solid piece.

\begin{figure}[p]
	\centering
	\includegraphics[width=0.5\textwidth]{parts.jpg}
	\caption{3d printed parts \& on-board computer}
	\label{fig:printed-parts}
\end{figure} 

\begin{figure}[p]
	\centering
	\includegraphics[width=0.5\textwidth]{connection.png}
	\caption{The Raspberry Pi and power board connection.}
	\label{fig:connection}
\end{figure}  

Since all the components are available as shown 
in \cref{fig:components}, we could assemble 
all the parts in one piece and started testing the whole system.
We have used the zip tie to hold the components and to
make sure everything sticks together. Then we put them in the 3d
printed case. As shown in 
\cref{fig:full-hardware}, all the parts are in one system. 
While testing it, the drone struggled to take off and balance itself
because the total weight exceeded the limit. So we
only used the on that are marked with a green dot 
as shown in \cref{fig:components} and the weight reduced to \SI{193}{gram}. 
Now the drone takeoff is normal, but the drone faces difficulty moving normally ,so we removed all 3D parts, and now the drone moves normally.


\begin{figure}[p]
	\centering
	\includegraphics[width=0.5\textwidth]{components.jpg}
	\caption{All components needed.}
	\label{fig:components}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\textwidth]{fulldronehardware.png}
	\caption{Full hardware system.}
	\label{fig:full-hardware}
\end{figure}  



\subsection{Reinforcement learning}

The framework that we are proposing facilitates the training of
a Parrot drone to achieve a mission that contains uncertainty
based on an arbitrary distribution. 
In our case, the drone is the Parrot Anafi and 
the mission is to complete a 
mobile target visitation task 
in the shortest time and least energy.
The uncertainty of the mission comes from the initial positions and 
movements of the targets.

The initial positions of the targets are generated from 
a skewed
multivariate normal distribution shown in Fig.~\ref{fig:position-distribution}.
After every episode, new positions are generated.
In addition, their movements consist of eight discrete
actions: forward, backward, left, right,
forward-left, forward-right, backward-left and backward-right.
Each movement lasts for 1 meter, and then a new one is
chosen.
We have made the targets such that they choose to move 70\% 
of the time forward-left and 30\% of the time divided
among the remaining seven movements.

\begin{figure}[!t]
	\centering
	\includegraphics[width=3.4in]{target-distribution}
	\caption{The distribution of the initial positions of the
		targets. The x-y plane corresponds to the ground, and 
		the drone faces the positive x-direction throughout the mission.}
	\label{fig:position-distribution}
\end{figure}

To train a drone using RL requires the
design of the environment, the agent itself and
the interaction between them as shown in Fig.~\ref{fig:rl}. 
The agent, due to being in a certain state, chooses and 
performs an action on
the environment resulting in the environment 
rewarding the agent for taking the action in that state
and then replying to it with a next state.

\begin{figure}[!t]
	\centering
	\includegraphics[width=3.4in]{rl-flowchart}
	\caption{The interaction between the agent and environment
		in reinforcement learning \cite{Sut20}.}
	\label{fig:rl}
\end{figure}

In our framework, we have used the Parrot Sphinx simulator
as the environment and a simulated Anafi drone as the agent.
The simulated drone extends the OpenAI Gym \texttt{Env} 
Python class
to inherit the methods every RL agent
needs to undergo to learn.
The exchange of state, reward and action between them 
occur through the Parrot Olympe controller as well as
through JSON-RPC and message passing protocols.
More detailed commands used and steps taken are laid out 
in Fig.~\ref{fig:flowchart}.

\begin{figure}[!t]
	\centering
	\includegraphics[width=3.4in]{sim-flowchart}
	\caption{Algorithm implementation: commands and messages passing
		between programs to train the drone.}
	\label{fig:flowchart}
\end{figure}

The correct reward and state representations are critical
to making the drone learn to visit the targets in the shortest
time and with the least energy.
The ground is divided into 25 cells as illustrated in 
Fig.~\ref{fig:grid}, and in each timestep,
the drone can occupy only one of those cells.
In addition, each target is given a unique ID which 
is embedded in the QR code that the targets are covered with
as shown in Fig.~\ref{fig:target}.

The drone has nine discrete actions, the first eight of which
are similar to the targets.
They are forward, backward, left, right,
forward-left, forward-right, backward-left, backward-right
and hover. 
Another difference between the drone and target movements 
besides the hovering action  
is the distance travelled -- while the targets move for 
1 meter before changing direction, the drone moves to 
one of the neighbouring eight cells or,
in case of the hovering action, the current cell.

\begin{figure}[!t]
	\centering
	\includegraphics[width=3.4in]{grid}
	\caption{The environment of the agent with its ground
		divided into 25 cells.}
	\label{fig:grid}
\end{figure}

\begin{figure}[!t]
	\centering
	\includegraphics[width=3.4in]{target-3}
	\caption{Example of a target with ID 3 in the environment. The QR code
		embedding the ID information covers the top side of the target.}
	\label{fig:target}
\end{figure}

We have set the reward to be equal to 1.5 multiplied by 
the number of new targets captured in the new state.
If there is no new target, the reward is -1. 
This ensures the drone learns to
minimise the time.
Since there are 10 targets, it follows that the maximum
return (sum of rewards in an episode) that the agent can gain  
in an episode is 15.
To obtain the number of new targets, the drone's simulated camera is
used to take a photo of the cell
underneath, which is then processed by a QR code detection 
model, Pyzbar. 
The outputs of the processing are the IDs of 
the detected targets, 
of which only the previously not encountered ones during 
the episode are counted towards the reward.

On the other hand, the state space is made up of several 
pieces of information as follows
\begin{align}
	s = \{ t, cell, [ I_1, I_2, I_3, \ldots, I_m] \} 
	\label{eq:state-space}
\end{align}

\noindent 
where $t$ is the current time step, $cell$ is the ID of the
cell underneath the drone, $I_k$ is the binary variable 
that indicates if the target with an ID of $k$ has been
visited, $m$ is the total number of targets,
and the vector of length $m$ is the container for the
$I$ indicators. 
For example,
if only targets with ID's 3 and 7 have been visited
in the current and previous time steps since the beginning of the 
episode,
then the vector will have elements of one for $I_3$ and $I_7$ while
the other elements are zeros.

\subsection{User interface}

\begin{figure}[tbp] 
	\centering
	\includegraphics[width=0.9\textwidth]{gui-use-cases} 
	\caption{This use case diagram depicts the different use cases that any authorized users can have using the website.}
	\label{fig:gui-use-cases} 
\end{figure}

Below are the list of use cases that a user can do using the AirEye website as seen in \cref{fig:gui-use-cases} : 
\begin{itemize}	
	\item The user is able to choose a certain model that is available, choosing this model will run a specific script in the drone. 
	\item The user can start the mission.
	\item The user can watch the live stream from the drone camera and refresh the stream in case of any error.
	\item The user can see the outputs of the terminal to keep track of different variables.
	\item The user can save the stream recording. 
	\item The user can end the mission.
\end{itemize}

\begin{figure}[tbp]
	\centering
	\includegraphics[width=0.99\textwidth]{gui-main-page}
	\caption{The main page of the AirEye website.}
	\label{fig:gui-main-page}
\end{figure}

Many software components were put together to produce what we have in \cref{fig:gui-main-page}, mainly: 
\begin{itemize}
	\item Olympe, the drone controller. 
	\item pyzbar, for \textsc{qr} code detection.
	\item OpenCV, huge computer vision library.
	\item \textsc{html}, \textsc{css}, and javascript for frontend web 
	development.
	\item Python Flask, backend framework.
	\item Gunicorn, \textsc{wsgi} \textsc{http} server
	\item Nginx, reverse proxy and web server. 
\end{itemize}

Many novel aspects were implemented in this website, different models could be 
made and each model is responsible for a certain type of targets, One model 
might be good for mobile targets and another could be better for fixed ones. 
Challenges were mainly faced in the backend as establishing the communication 
between the web server and the frontend was not easy to manage. Also, 
streaming live from the drone's camera to the user interface was 
difficult and new to us.



\end{document}
